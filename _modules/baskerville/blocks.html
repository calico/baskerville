

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>baskerville.blocks &mdash; baskerville 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            baskerville
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../baskerville.html">baskerville package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">baskerville</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">baskerville.blocks</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for baskerville.blocks</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2019 Calico LLC</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># =========================================================================</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pdb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">baskerville</span><span class="w"> </span><span class="kn">import</span> <span class="n">layers</span>


<span class="c1">############################################################</span>
<span class="c1"># Convolution</span>
<span class="c1">############################################################</span>
<div class="viewcode-block" id="conv_block">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_block">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_block</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">activation_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dilation_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters:       Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      stride:        Conv1D stride</span>
<span class="sd">      dilation_rate: Conv1D dilation rate</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      conv_type:     Conv1D layer type</span>
<span class="sd">      residual:      Residual connection boolean</span>
<span class="sd">      pool_size:     Max pool width</span>
<span class="sd">      norm_type:     Apply batch or layer normalization</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>
<span class="sd">      norm_gamma:    BatchNorm gamma (defaults according to residual)</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># choose convolution type</span>
    <span class="k">if</span> <span class="n">conv_type</span> <span class="o">==</span> <span class="s2">&quot;separable&quot;</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv1D</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span>

    <span class="k">if</span> <span class="n">filters</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="p">(</span><span class="n">norm_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">dilation_rate</span><span class="o">=</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">)(</span>
            <span class="n">current</span>
        <span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual add</span>
    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="c1"># end activation</span>
    <span class="k">if</span> <span class="n">activation_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation_end</span><span class="p">)</span>

    <span class="c1"># Pool</span>
    <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pool_type</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftmaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)(</span>
                <span class="n">current</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="conv_dna">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_dna">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_dna</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout_residual</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">se</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single convolution block, assumed to be operating on DNA.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters:       Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      stride:        Conv1D stride</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      conv_type:     Conv1D layer type</span>
<span class="sd">      pool_size:     Max pool width</span>
<span class="sd">      norm_type:     Apply batch or layer normalization</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># choose convolution type</span>
    <span class="k">if</span> <span class="n">conv_type</span> <span class="o">==</span> <span class="s2">&quot;separable&quot;</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv1D</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span>

    <span class="k">if</span> <span class="n">filters</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># need option to define for older models</span>
    <span class="k">if</span> <span class="n">use_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_bias</span> <span class="o">=</span> <span class="n">norm_type</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">residual</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># squeeze-excite</span>
    <span class="k">if</span> <span class="n">se</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">squeeze_excite</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="c1"># residual conv block</span>
        <span class="n">rcurrent</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">l2_scale</span><span class="o">=</span><span class="n">l2_scale</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_residual</span><span class="p">,</span>
            <span class="n">conv_type</span><span class="o">=</span><span class="n">conv_type</span><span class="p">,</span>
            <span class="n">norm_type</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="n">se</span><span class="o">=</span><span class="n">se</span><span class="p">,</span>
            <span class="n">bn_momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># residual add</span>
        <span class="n">rcurrent</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Scale</span><span class="p">()(</span><span class="n">rcurrent</span><span class="p">)</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">current</span><span class="p">,</span> <span class="n">rcurrent</span><span class="p">])</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># normalize</span>
        <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
                <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># activation</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># Pool</span>
    <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pool_type</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftmaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)(</span>
                <span class="n">current</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="conv_nac">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_nac">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_nac</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dilation_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">se</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters:       Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      stride:        Conv1D stride</span>
<span class="sd">      dilation_rate: Conv1D dilation rate</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      conv_type:     Conv1D layer type</span>
<span class="sd">      residual:      Residual connection boolean</span>
<span class="sd">      pool_size:     Max pool width</span>
<span class="sd">      norm_type:     Apply batch or layer normalization</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># choose convolution type</span>
    <span class="k">if</span> <span class="n">conv_type</span> <span class="o">==</span> <span class="s2">&quot;separable&quot;</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv1D</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span>

    <span class="k">if</span> <span class="n">filters</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation_rate</span><span class="o">=</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># squeeze-excite</span>
    <span class="k">if</span> <span class="n">se</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">squeeze_excite</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual add</span>
    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="c1"># Pool</span>
    <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pool_type</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftmaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)(</span>
                <span class="n">current</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="conv_next">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_next">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_next</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">dense_expansion</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">dilation_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters:       Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      dilation_rate: Conv1D dilation rate</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      residual:      Residual connection boolean</span>
<span class="sd">      pool_size:     Max pool width</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">filters</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv1D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation_rate</span><span class="o">=</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># normalize</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dense expansion</span>
    <span class="n">expansion_filters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dense_expansion</span><span class="p">)</span> <span class="o">*</span> <span class="n">filters</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">expansion_filters</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># dense contraction</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual add</span>
    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="c1"># Pool</span>
    <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pool_type</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftmaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)(</span>
                <span class="n">current</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="unet_conv">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.unet_conv">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">unet_conv</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">unet_repr</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">upsample_conv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a feature pyramid network block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      stride:        UpSample stride</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      norm_type:     Apply batch or layer normalization</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>
<span class="sd">      upsample_conv: Conv1D the upsampled input path</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># variables</span>
    <span class="n">current1</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">current2</span> <span class="o">=</span> <span class="n">unet_repr</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current1</span><span class="p">)</span>
        <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current1</span><span class="p">)</span>
        <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current1</span><span class="p">)</span>
        <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current2</span><span class="p">)</span>

    <span class="c1"># activate</span>
    <span class="n">current1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current1</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>
    <span class="n">current2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current2</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="n">filters</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># dense</span>
    <span class="k">if</span> <span class="n">upsample_conv</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">units</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">current1</span><span class="p">)</span>
    <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current2</span><span class="p">)</span>

    <span class="c1"># upsample</span>
    <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">UpSampling1D</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">stride</span><span class="p">)(</span><span class="n">current1</span><span class="p">)</span>

    <span class="c1"># add</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">current1</span><span class="p">,</span> <span class="n">current2</span><span class="p">])</span>

    <span class="c1"># normalize?</span>
    <span class="c1"># activate?</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv1D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="unet_concat">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.unet_concat">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">unet_concat</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">unet_repr</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single transposed convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters:       Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      stride:        UpSample stride</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      conv_type:     Conv1D layer type</span>
<span class="sd">      norm_type:     Apply batch or layer normalization</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, stride*seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># variables</span>
    <span class="n">current1</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">current2</span> <span class="o">=</span> <span class="n">unet_repr</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current1</span><span class="p">)</span>
        <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current1</span><span class="p">)</span>
        <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current1</span><span class="p">)</span>
        <span class="n">current2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current2</span><span class="p">)</span>

    <span class="c1"># upsample</span>
    <span class="n">current1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">UpSampling1D</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">stride</span><span class="p">)(</span><span class="n">current1</span><span class="p">)</span>

    <span class="c1"># concatenate</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()([</span><span class="n">current2</span><span class="p">,</span> <span class="n">current1</span><span class="p">])</span>

    <span class="c1"># activate</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">mid_units</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">unet_repr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">mid_units</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># activate</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">unet_repr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">unet_repr</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="tconv_nac">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.tconv_nac">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">tconv_nac</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single transposed convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters:       Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      activation:    relu/gelu/etc</span>
<span class="sd">      stride:        UpSample stride</span>
<span class="sd">      l2_scale:      L2 regularization weight.</span>
<span class="sd">      dropout:       Dropout rate probability</span>
<span class="sd">      conv_type:     Conv1D layer type</span>
<span class="sd">      norm_type:     Apply batch or layer normalization</span>
<span class="sd">      bn_momentum:   BatchNorm momentum</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, stride*seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="k">if</span> <span class="n">filters</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1DTranspose</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="conv_block_2d">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_block_2d">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_block_2d</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dilation_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single 2D convolution block.&quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># choose convolution type</span>
    <span class="k">if</span> <span class="n">conv_type</span> <span class="o">==</span> <span class="s2">&quot;separable&quot;</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv2D</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span>

    <span class="c1"># convolution</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="p">(</span><span class="n">norm_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">dilation_rate</span><span class="o">=</span><span class="n">dilation_rate</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">)(</span>
            <span class="n">current</span>
        <span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># pool</span>
    <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)(</span>
            <span class="n">current</span>
        <span class="p">)</span>

    <span class="c1"># symmetric</span>
    <span class="k">if</span> <span class="n">symmetric</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Symmetrize2D</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Towers</span>
<span class="c1">############################################################</span>
<div class="viewcode-block" id="conv_tower_v1">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_tower_v1">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_tower_v1</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters_init</span><span class="p">,</span> <span class="n">filters_mult</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a reducing convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters_init:  Initial Conv1D filters</span>
<span class="sd">      filters_mult:  Multiplier for Conv1D filters</span>
<span class="sd">      repeat:        Conv block repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize filters</span>
    <span class="n">rep_filters</span> <span class="o">=</span> <span class="n">filters_init</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="c1"># convolution</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">rep_filters</span><span class="p">)),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># update filters</span>
        <span class="n">rep_filters</span> <span class="o">*=</span> <span class="n">filters_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="conv_tower">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_tower">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_tower</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters_init</span><span class="p">,</span>
    <span class="n">filters_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">filters_mult</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">divisible_by</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">reprs</span><span class="o">=</span><span class="p">[],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a reducing convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters_init:  Initial Conv1D filters</span>
<span class="sd">      filters_end:   End Conv1D filters</span>
<span class="sd">      filters_mult:  Multiplier for Conv1D filters</span>
<span class="sd">      divisible_by:  Round filters to be divisible by (eg a power of two)</span>
<span class="sd">      repeat:        Tower repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">divisible_by</span><span class="p">)</span> <span class="o">*</span> <span class="n">divisible_by</span><span class="p">)</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize filters</span>
    <span class="n">rep_filters</span> <span class="o">=</span> <span class="n">filters_init</span>

    <span class="c1"># determine multiplier</span>
    <span class="k">if</span> <span class="n">filters_mult</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">filters_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">filters_mult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">filters_end</span> <span class="o">/</span> <span class="n">filters_init</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">repeat</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="c1"># convolution</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">_round</span><span class="p">(</span><span class="n">rep_filters</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># save representation</span>
        <span class="n">reprs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># update filters</span>
        <span class="n">rep_filters</span> <span class="o">*=</span> <span class="n">filters_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="conv_tower_nac">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.conv_tower_nac">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_tower_nac</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters_init</span><span class="p">,</span>
    <span class="n">filters_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">filters_mult</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">divisible_by</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">reprs</span><span class="o">=</span><span class="p">[],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a reducing convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters_init:  Initial Conv1D filters</span>
<span class="sd">      filters_end:   End Conv1D filters</span>
<span class="sd">      filters_mult:  Multiplier for Conv1D filters</span>
<span class="sd">      divisible_by:  Round filters to be divisible by (eg a power of two)</span>
<span class="sd">      repeat:        Tower repetitions</span>
<span class="sd">      reprs:         Append representations.</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">divisible_by</span><span class="p">)</span> <span class="o">*</span> <span class="n">divisible_by</span><span class="p">)</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize filters</span>
    <span class="n">rep_filters</span> <span class="o">=</span> <span class="n">filters_init</span>

    <span class="c1"># determine multiplier</span>
    <span class="k">if</span> <span class="n">filters_mult</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">filters_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">filters_mult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">filters_end</span> <span class="o">/</span> <span class="n">filters_init</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">repeat</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="c1"># convolution</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">_round</span><span class="p">(</span><span class="n">rep_filters</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># save representation</span>
        <span class="n">reprs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># update filters</span>
        <span class="n">rep_filters</span> <span class="o">*=</span> <span class="n">filters_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="res_tower">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.res_tower">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">res_tower</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters_init</span><span class="p">,</span>
    <span class="n">filters_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">filters_mult</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">divisible_by</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_convs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">reprs</span><span class="o">=</span><span class="p">[],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a reducing convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters_init:  Initial Conv1D filters</span>
<span class="sd">      filters_end:   End Conv1D filters</span>
<span class="sd">      filters_mult:  Multiplier for Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      dropout:       Dropout on subsequent convolution blocks.</span>
<span class="sd">      pool_size:     Pool width.</span>
<span class="sd">      repeat:        Residual block repetitions</span>
<span class="sd">      num_convs:     Conv blocks per residual layer</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">divisible_by</span><span class="p">)</span> <span class="o">*</span> <span class="n">divisible_by</span><span class="p">)</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize filters</span>
    <span class="n">rep_filters</span> <span class="o">=</span> <span class="n">filters_init</span>

    <span class="c1"># determine multiplier</span>
    <span class="k">if</span> <span class="n">filters_mult</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">filters_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">filters_mult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">filters_end</span> <span class="o">/</span> <span class="n">filters_init</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">repeat</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">rep_filters_int</span> <span class="o">=</span> <span class="n">_round</span><span class="p">(</span><span class="n">rep_filters</span><span class="p">)</span>

        <span class="c1"># initial</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">rep_filters_int</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
        <span class="n">current0</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># subsequent</span>
        <span class="k">for</span> <span class="n">ci</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_convs</span><span class="p">):</span>
            <span class="c1"># bg = &#39;ones&#39; if ci &lt; num_convs-1 else &#39;zeros&#39;</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">rep_filters_int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># dropout</span>
        <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># residual add</span>
        <span class="k">if</span> <span class="n">num_convs</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Scale</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">current0</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

        <span class="c1"># save representation</span>
        <span class="n">reprs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># pool</span>
        <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pool_type</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftmaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool1D</span><span class="p">(</span>
                    <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span>
                <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># update filters</span>
        <span class="n">rep_filters</span> <span class="o">*=</span> <span class="n">filters_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="convnext_tower">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.convnext_tower">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">convnext_tower</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters_init</span><span class="p">,</span>
    <span class="n">filters_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">filters_mult</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">divisible_by</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_convs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">reprs</span><span class="o">=</span><span class="p">[],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abc.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      filters_init:  Initial Conv1D filters</span>
<span class="sd">      filters_end:   End Conv1D filters</span>
<span class="sd">      filters_mult:  Multiplier for Conv1D filters</span>
<span class="sd">      kernel_size:   Conv1D kernel_size</span>
<span class="sd">      dropout:       Dropout on subsequent convolution blocks.</span>
<span class="sd">      pool_size:     Pool width.</span>
<span class="sd">      repeat:        Residual block repetitions</span>
<span class="sd">      num_convs:     Conv blocks per residual layer</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">divisible_by</span><span class="p">)</span> <span class="o">*</span> <span class="n">divisible_by</span><span class="p">)</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize filters</span>
    <span class="n">rep_filters</span> <span class="o">=</span> <span class="n">filters_init</span>

    <span class="c1"># determine multiplier</span>
    <span class="k">if</span> <span class="n">filters_mult</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">filters_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">filters_mult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">filters_end</span> <span class="o">/</span> <span class="n">filters_init</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">repeat</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">rep_filters_int</span> <span class="o">=</span> <span class="n">_round</span><span class="p">(</span><span class="n">rep_filters</span><span class="p">)</span>

        <span class="c1"># initial</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_next</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">rep_filters_int</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">current0</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># subsequent</span>
        <span class="k">for</span> <span class="n">ci</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_convs</span><span class="p">):</span>
            <span class="c1"># bg = &#39;ones&#39; if ci &lt; num_convs-1 else &#39;zeros&#39;</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">conv_next</span><span class="p">(</span>
                <span class="n">current</span><span class="p">,</span>
                <span class="n">filters</span><span class="o">=</span><span class="n">rep_filters_int</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># residual add</span>
        <span class="k">if</span> <span class="n">num_convs</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Scale</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">current0</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

        <span class="c1"># pool</span>
        <span class="k">if</span> <span class="n">pool_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pool_type</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftmaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool1D</span><span class="p">(</span>
                    <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span>
                <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># save representation</span>
        <span class="n">reprs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># update filters</span>
        <span class="n">rep_filters</span> <span class="o">*=</span> <span class="n">filters_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Attention</span>
<span class="c1">############################################################</span>
<div class="viewcode-block" id="transformer">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.transformer">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">transformer</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">key_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">out_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">dense_expansion</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">content_position_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">position_dropout</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">mha_l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">num_position_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">qkv_width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">mha_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">seqlen_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a transformer block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      key_size:        Conv block repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">out_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">out_size</span> <span class="o">%</span> <span class="n">heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">value_size</span> <span class="o">=</span> <span class="n">out_size</span> <span class="o">//</span> <span class="n">heads</span>

    <span class="c1"># layer norm</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># multi-head attention</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">value_size</span><span class="o">=</span><span class="n">value_size</span><span class="p">,</span>
        <span class="n">key_size</span><span class="o">=</span><span class="n">key_size</span><span class="p">,</span>
        <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
        <span class="n">num_position_features</span><span class="o">=</span><span class="n">num_position_features</span><span class="p">,</span>
        <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
        <span class="n">positional_dropout_rate</span><span class="o">=</span><span class="n">position_dropout</span><span class="p">,</span>
        <span class="n">content_position_bias</span><span class="o">=</span><span class="n">content_position_bias</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">mha_initializer</span><span class="p">,</span>
        <span class="n">l2_scale</span><span class="o">=</span><span class="n">mha_l2_scale</span><span class="p">,</span>
        <span class="n">qkv_width</span><span class="o">=</span><span class="n">qkv_width</span><span class="p">,</span>
        <span class="n">seqlen_train</span><span class="o">=</span><span class="n">seqlen_train</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">dense_expansion</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">final</span> <span class="o">=</span> <span class="n">current</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final</span> <span class="o">=</span> <span class="n">transformer_dense</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">dense_expansion</span><span class="p">,</span> <span class="n">l2_scale</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">kernel_initializer</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">final</span></div>



<div class="viewcode-block" id="transformer_split">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.transformer_split">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">transformer_split</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">key_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">out_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">dense_expansion</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">content_position_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">position_dropout</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">mha_l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">num_position_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">qkv_width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">mha_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a transformer block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      key_size:        Conv block repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">out_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">out_size</span> <span class="o">%</span> <span class="n">heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">value_size</span> <span class="o">=</span> <span class="n">out_size</span> <span class="o">//</span> <span class="n">heads</span>

    <span class="c1"># layer norm</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># multi-head attention</span>
    <span class="n">mha</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">value_size</span><span class="o">=</span><span class="n">value_size</span><span class="p">,</span>
        <span class="n">key_size</span><span class="o">=</span><span class="n">key_size</span><span class="p">,</span>
        <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
        <span class="n">num_position_features</span><span class="o">=</span><span class="n">num_position_features</span><span class="p">,</span>
        <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
        <span class="n">positional_dropout_rate</span><span class="o">=</span><span class="n">position_dropout</span><span class="p">,</span>
        <span class="n">content_position_bias</span><span class="o">=</span><span class="n">content_position_bias</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">mha_initializer</span><span class="p">,</span>
        <span class="n">l2_scale</span><span class="o">=</span><span class="n">mha_l2_scale</span><span class="p">,</span>
        <span class="n">qkv_width</span><span class="o">=</span><span class="n">qkv_width</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_depth</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">seq_len2</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">seq_len4</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="mi">4</span>

    <span class="k">if</span> <span class="n">splits</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># split in two length-wise</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">seq_len2</span><span class="p">,</span> <span class="n">seq_depth</span><span class="p">))(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># MHA left/right</span>
        <span class="n">current_left</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">current</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
        <span class="n">current_right</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">current</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>

        <span class="n">current_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_left</span><span class="p">,</span> <span class="n">current_right</span><span class="p">]</span>

    <span class="k">elif</span> <span class="n">splits</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># split in four length-wise</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_len4</span><span class="p">,</span> <span class="n">seq_depth</span><span class="p">))(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># transformer left/right</span>
        <span class="n">current_left</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">current</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
        <span class="n">current_right</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">current</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>

        <span class="c1"># transformer center</span>
        <span class="n">current_center</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span><span class="n">seq_len2</span><span class="p">,</span> <span class="n">seq_depth</span><span class="p">))(</span>
            <span class="n">current</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="p">)</span>
        <span class="n">current_center</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">current_center</span><span class="p">)</span>

        <span class="n">current_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_left</span><span class="p">,</span> <span class="n">current_center</span><span class="p">,</span> <span class="n">current_right</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transformer_split not implemented for splits &gt; 3&quot;</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># concat along position axis</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">current_list</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">dense_expansion</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">final</span> <span class="o">=</span> <span class="n">current</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">final</span> <span class="o">=</span> <span class="n">transformer_dense</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">dense_expansion</span><span class="p">,</span> <span class="n">l2_scale</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">kernel_initializer</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">final</span></div>



<div class="viewcode-block" id="transformer_dense">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.transformer_dense">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">transformer_dense</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">dense_expansion</span><span class="p">,</span> <span class="n">l2_scale</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">kernel_initializer</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformer block dense portion.&quot;&quot;&quot;</span>
    <span class="c1"># layer norm</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">expansion_filters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dense_expansion</span> <span class="o">*</span> <span class="n">out_size</span><span class="p">)</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">expansion_filters</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">out_size</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">l2_scale</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual</span>
    <span class="n">final</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">final</span></div>



<div class="viewcode-block" id="transformer2">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.transformer2">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">transformer2</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">key_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">out_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">num_position_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">position_dropout</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">dense_expansion</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">qkv_width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a transformer block, with length-wise pooling before</span>
<span class="sd">       returning to full length.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      key_size:        Conv block repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">out_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">out_size</span> <span class="o">%</span> <span class="n">heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">value_size</span> <span class="o">=</span> <span class="n">out_size</span> <span class="o">//</span> <span class="n">heads</span>

    <span class="c1"># convolution to decrease length</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">filters</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">key_size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># layer norm</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># multi-head attention</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">value_size</span><span class="o">=</span><span class="n">value_size</span><span class="p">,</span>
        <span class="n">key_size</span><span class="o">=</span><span class="n">key_size</span><span class="p">,</span>
        <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span>
        <span class="n">num_position_features</span><span class="o">=</span><span class="n">num_position_features</span><span class="p">,</span>
        <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
        <span class="n">positional_dropout_rate</span><span class="o">=</span><span class="n">position_dropout</span><span class="p">,</span>
        <span class="n">transpose_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">qkv_width</span><span class="o">=</span><span class="n">qkv_width</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># concatenate and transform</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">out_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">dense_expansion</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">final</span> <span class="o">=</span> <span class="n">current</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">current_mha</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># layer norm</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># dense</span>
        <span class="n">expansion_filters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dense_expansion</span> <span class="o">*</span> <span class="n">out_size</span><span class="p">)</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">expansion_filters</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># dropout</span>
        <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># activation</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span>

        <span class="c1"># dense</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">out_size</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># dropout</span>
        <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># residual</span>
        <span class="n">final</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">current_mha</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">final</span></div>



<div class="viewcode-block" id="swin_transformer">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.swin_transformer">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">swin_transformer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">transformer_split</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">transformer_split</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="transformer_tower">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.transformer_tower">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">transformer_tower</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">block_type</span><span class="o">=</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a tower of repeated transformer blocks.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:        [batch_size, seq_length, features] input sequence</span>
<span class="sd">      repeat:        Conv block repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length, features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;lambda&quot;</span><span class="p">:</span>
        <span class="n">transformer_block</span> <span class="o">=</span> <span class="n">transformer_lambda</span>
    <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;swin&quot;</span><span class="p">:</span>
        <span class="n">transformer_block</span> <span class="o">=</span> <span class="n">swin_transformer</span>
    <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;transformer2&quot;</span><span class="p">:</span>
        <span class="n">transformer_block</span> <span class="o">=</span> <span class="n">transformer2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">transformer_block</span> <span class="o">=</span> <span class="n">transformer</span>

    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="squeeze_excite">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.squeeze_excite">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">squeeze_excite</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">bottleneck_ratio</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">additive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">scale_fun</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">SqueezeExcite</span><span class="p">(</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">additive</span><span class="o">=</span><span class="n">additive</span><span class="p">,</span>
        <span class="n">bottleneck_ratio</span><span class="o">=</span><span class="n">bottleneck_ratio</span><span class="p">,</span>
        <span class="n">norm_type</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span>
        <span class="n">bn_momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">scale_fun</span><span class="o">=</span><span class="n">scale_fun</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">inputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="wheeze_excite">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.wheeze_excite">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">wheeze_excite</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">pool_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">WheezeExcite</span><span class="p">(</span><span class="n">pool_size</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="global_context">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.global_context">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">global_context</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalContext</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Dilated Towers</span>
<span class="c1">############################################################</span>


<div class="viewcode-block" id="dilated_dense">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.dilated_dense">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dilated_dense</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rate_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a residual dilated dense block.</span>

<span class="sd">    Args:</span>

<span class="sd">    Returns:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize dilation rate</span>
    <span class="n">dilation_rate</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">rep_input</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># dilate</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dilation_rate</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dilation_rate</span><span class="p">)),</span>
            <span class="n">conv_type</span><span class="o">=</span><span class="n">conv_type</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># dense concat</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()([</span><span class="n">rep_input</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

        <span class="c1"># update dilation rate</span>
        <span class="n">dilation_rate</span> <span class="o">*=</span> <span class="n">rate_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="dilated_residual">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.dilated_residual">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dilated_residual</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rate_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">conv_type</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="nb">round</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a residual dilated convolution block.</span>

<span class="sd">    Args:</span>

<span class="sd">    Returns:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize dilation rate</span>
    <span class="n">dilation_rate</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">rep_input</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># dilate</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dilation_rate</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dilation_rate</span><span class="p">)),</span>
            <span class="n">conv_type</span><span class="o">=</span><span class="n">conv_type</span><span class="p">,</span>
            <span class="n">norm_type</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="n">norm_gamma</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># return</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">rep_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">norm_type</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="n">norm_gamma</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># InitZero</span>
        <span class="k">if</span> <span class="n">norm_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Scale</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># residual add</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">rep_input</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

        <span class="c1"># update dilation rate</span>
        <span class="n">dilation_rate</span> <span class="o">*=</span> <span class="n">rate_mult</span>
        <span class="k">if</span> <span class="nb">round</span><span class="p">:</span>
            <span class="n">dilation_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dilation_rate</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="dilated_residual_nac">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.dilated_residual_nac">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dilated_residual_nac</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rate_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a residual dilated convolution block.</span>

<span class="sd">    Args:</span>

<span class="sd">    Returns:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize dilation rate</span>
    <span class="n">dilation_rate</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">rep_input</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># dilate</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dilation_rate</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dilation_rate</span><span class="p">)),</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># return</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_nac</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">rep_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="c1"># residual add</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">rep_input</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

        <span class="c1"># update dilation rate</span>
        <span class="n">dilation_rate</span> <span class="o">*=</span> <span class="n">rate_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="dilated_residual_2d">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.dilated_residual_2d">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dilated_residual_2d</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">filters</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rate_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a residual dilated convolution block.&quot;&quot;&quot;</span>

    <span class="c1"># flow through variable current</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># initialize dilation rate</span>
    <span class="n">dilation_rate</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeat</span><span class="p">):</span>
        <span class="n">rep_input</span> <span class="o">=</span> <span class="n">current</span>

        <span class="c1"># dilate</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block_2d</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dilation_rate</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">dilation_rate</span><span class="p">)),</span>
            <span class="n">norm_gamma</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># return</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">conv_block_2d</span><span class="p">(</span>
            <span class="n">current</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">rep_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">norm_gamma</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># residual add</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">rep_input</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

        <span class="c1"># enforce symmetry</span>
        <span class="k">if</span> <span class="n">symmetric</span><span class="p">:</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Symmetrize2D</span><span class="p">()(</span><span class="n">current</span><span class="p">)</span>

        <span class="c1"># update dilation rate</span>
        <span class="n">dilation_rate</span> <span class="o">*=</span> <span class="n">rate_mult</span>

    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Center ops</span>
<span class="c1">############################################################</span>


<div class="viewcode-block" id="center_average">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.center_average">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">center_average</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">CenterAverage</span><span class="p">(</span><span class="n">center</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="center_slice">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.center_slice">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">center_slice</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">CenterSlice</span><span class="p">(</span><span class="n">center</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># 2D</span>
<span class="c1">############################################################</span>


<div class="viewcode-block" id="concat_dist_2d">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.concat_dist_2d">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">concat_dist_2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">ConcatDist2D</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="concat_position">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.concat_position">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">concat_position</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="s2">&quot;abs&quot;</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">ConcatPosition</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">power</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="cropping_2d">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.cropping_2d">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">cropping_2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cropping</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Cropping2D</span><span class="p">(</span><span class="n">cropping</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="one_to_two">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.one_to_two">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">one_to_two</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">operation</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">OneToTwo</span><span class="p">(</span><span class="n">operation</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="symmetrize_2d">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.symmetrize_2d">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">symmetrize_2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Symmetrize2D</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="upper_tri">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.upper_tri">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">upper_tri</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">diagonal_offset</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">UpperTri</span><span class="p">(</span><span class="n">diagonal_offset</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Factorization</span>
<span class="c1">############################################################</span>


<div class="viewcode-block" id="factor_inverse">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.factor_inverse">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">factor_inverse</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">components_file</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">FactorInverse</span><span class="p">(</span><span class="n">components_file</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Dense</span>
<span class="c1">############################################################</span>
<div class="viewcode-block" id="dense_block">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.dense_block">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dense_block</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">activation_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l1_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:         [batch_size, seq_length, features] input sequence</span>
<span class="sd">      units:          Conv1D filters</span>
<span class="sd">      activation:     relu/gelu/etc</span>
<span class="sd">      activation_end: Compute activation after the other operations</span>
<span class="sd">      flatten:        Flatten across positional axis</span>
<span class="sd">      dropout:        Dropout rate probability</span>
<span class="sd">      l2_scale:       L2 regularization weight.</span>
<span class="sd">      l1_scale:       L1 regularization weight.</span>
<span class="sd">      residual:       Residual connection boolean</span>
<span class="sd">      batch_norm:     Apply batch normalization</span>
<span class="sd">      bn_momentum:    BatchNorm momentum</span>
<span class="sd">      norm_gamma:       BatchNorm gamma (defaults according to residual)</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length(?), features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="k">if</span> <span class="n">units</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">units</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># flatten</span>
    <span class="k">if</span> <span class="n">flatten</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_depth</span> <span class="o">=</span> <span class="n">current</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="mi">1</span><span class="p">,</span>
                <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_depth</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="p">(</span><span class="n">norm_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l1_l2</span><span class="p">(</span><span class="n">l1_scale</span><span class="p">,</span> <span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">norm_gamma</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span> <span class="k">if</span> <span class="n">residual</span> <span class="k">else</span> <span class="s2">&quot;ones&quot;</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">)(</span>
            <span class="n">current</span>
        <span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual add</span>
    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="c1"># end activation</span>
    <span class="k">if</span> <span class="n">activation_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation_end</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="dense_nac">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.dense_nac">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">dense_nac</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l1_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bn_momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">norm_gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a single convolution block.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:         [batch_size, seq_length, features] input sequence</span>
<span class="sd">      units:          Conv1D filters</span>
<span class="sd">      activation:     relu/gelu/etc</span>
<span class="sd">      activation_end: Compute activation after the other operations</span>
<span class="sd">      flatten:        Flatten across positional axis</span>
<span class="sd">      dropout:        Dropout rate probability</span>
<span class="sd">      l2_scale:       L2 regularization weight.</span>
<span class="sd">      l1_scale:       L1 regularization weight.</span>
<span class="sd">      residual:       Residual connection boolean</span>
<span class="sd">      batch_norm:     Apply batch normalization</span>
<span class="sd">      bn_momentum:    BatchNorm momentum</span>
<span class="sd">      norm_gamma:       BatchNorm gamma (defaults according to residual)</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length(?), features] output sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="k">if</span> <span class="n">units</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">units</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">norm_gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">norm_gamma</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span> <span class="k">if</span> <span class="n">residual</span> <span class="k">else</span> <span class="s2">&quot;ones&quot;</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch-sync&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">,</span> <span class="n">synchronized</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">gamma_initializer</span><span class="o">=</span><span class="n">norm_gamma</span><span class="p">)(</span>
            <span class="n">current</span>
        <span class="p">)</span>

    <span class="c1"># activation</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">activate</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

    <span class="c1"># flatten</span>
    <span class="k">if</span> <span class="n">flatten</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_depth</span> <span class="o">=</span> <span class="n">current</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="mi">1</span><span class="p">,</span>
                <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_depth</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l1_l2</span><span class="p">(</span><span class="n">l1_scale</span><span class="p">,</span> <span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dropout</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">dropout</span><span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># residual add</span>
    <span class="k">if</span> <span class="n">residual</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">current</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">current</span></div>



<div class="viewcode-block" id="final">
<a class="viewcode-back" href="../../baskerville.html#baskerville.blocks.final">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">final</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">units</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
    <span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
    <span class="n">l2_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l1_scale</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Final simple transformation before comparison to targets.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs:         [batch_size, seq_length, features] input sequence</span>
<span class="sd">      units:          Dense units</span>
<span class="sd">      activation:     relu/gelu/etc</span>
<span class="sd">      flatten:        Flatten positional axis.</span>
<span class="sd">      l2_scale:       L2 regularization weight.</span>
<span class="sd">      l1_scale:       L1 regularization weight.</span>

<span class="sd">    Returns:</span>
<span class="sd">      [batch_size, seq_length(?), units] output sequence</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># flatten</span>
    <span class="k">if</span> <span class="n">flatten</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_depth</span> <span class="o">=</span> <span class="n">current</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="mi">1</span><span class="p">,</span>
                <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_depth</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="c1"># dense</span>
    <span class="n">current</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l1_l2</span><span class="p">(</span><span class="n">l1_scale</span><span class="p">,</span> <span class="n">l2_scale</span><span class="p">),</span>
    <span class="p">)(</span><span class="n">current</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">current</span></div>



<span class="c1">############################################################</span>
<span class="c1"># Dictionary</span>
<span class="c1">############################################################</span>
<span class="n">name_func</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;center_slice&quot;</span><span class="p">:</span> <span class="n">center_slice</span><span class="p">,</span>
    <span class="s2">&quot;center_average&quot;</span><span class="p">:</span> <span class="n">center_average</span><span class="p">,</span>
    <span class="s2">&quot;concat_dist_2d&quot;</span><span class="p">:</span> <span class="n">concat_dist_2d</span><span class="p">,</span>
    <span class="s2">&quot;concat_position&quot;</span><span class="p">:</span> <span class="n">concat_position</span><span class="p">,</span>
    <span class="s2">&quot;conv_block&quot;</span><span class="p">:</span> <span class="n">conv_block</span><span class="p">,</span>
    <span class="s2">&quot;conv_dna&quot;</span><span class="p">:</span> <span class="n">conv_dna</span><span class="p">,</span>
    <span class="s2">&quot;conv_nac&quot;</span><span class="p">:</span> <span class="n">conv_nac</span><span class="p">,</span>
    <span class="s2">&quot;conv_next&quot;</span><span class="p">:</span> <span class="n">conv_next</span><span class="p">,</span>
    <span class="s2">&quot;conv_block_2d&quot;</span><span class="p">:</span> <span class="n">conv_block_2d</span><span class="p">,</span>
    <span class="s2">&quot;conv_tower&quot;</span><span class="p">:</span> <span class="n">conv_tower</span><span class="p">,</span>
    <span class="s2">&quot;conv_tower_nac&quot;</span><span class="p">:</span> <span class="n">conv_tower_nac</span><span class="p">,</span>
    <span class="s2">&quot;convnext_tower&quot;</span><span class="p">:</span> <span class="n">convnext_tower</span><span class="p">,</span>
    <span class="s2">&quot;cropping_2d&quot;</span><span class="p">:</span> <span class="n">cropping_2d</span><span class="p">,</span>
    <span class="s2">&quot;dense_block&quot;</span><span class="p">:</span> <span class="n">dense_block</span><span class="p">,</span>
    <span class="s2">&quot;dense_nac&quot;</span><span class="p">:</span> <span class="n">dense_nac</span><span class="p">,</span>
    <span class="s2">&quot;dilated_residual&quot;</span><span class="p">:</span> <span class="n">dilated_residual</span><span class="p">,</span>
    <span class="s2">&quot;dilated_residual_nac&quot;</span><span class="p">:</span> <span class="n">dilated_residual_nac</span><span class="p">,</span>
    <span class="s2">&quot;dilated_residual_2d&quot;</span><span class="p">:</span> <span class="n">dilated_residual_2d</span><span class="p">,</span>
    <span class="s2">&quot;dilated_dense&quot;</span><span class="p">:</span> <span class="n">dilated_dense</span><span class="p">,</span>
    <span class="s2">&quot;factor_inverse&quot;</span><span class="p">:</span> <span class="n">factor_inverse</span><span class="p">,</span>
    <span class="s2">&quot;final&quot;</span><span class="p">:</span> <span class="n">final</span><span class="p">,</span>
    <span class="s2">&quot;global_context&quot;</span><span class="p">:</span> <span class="n">global_context</span><span class="p">,</span>
    <span class="s2">&quot;one_to_two&quot;</span><span class="p">:</span> <span class="n">one_to_two</span><span class="p">,</span>
    <span class="s2">&quot;symmetrize_2d&quot;</span><span class="p">:</span> <span class="n">symmetrize_2d</span><span class="p">,</span>
    <span class="s2">&quot;squeeze_excite&quot;</span><span class="p">:</span> <span class="n">squeeze_excite</span><span class="p">,</span>
    <span class="s2">&quot;swin_transformer&quot;</span><span class="p">:</span> <span class="n">swin_transformer</span><span class="p">,</span>
    <span class="s2">&quot;res_tower&quot;</span><span class="p">:</span> <span class="n">res_tower</span><span class="p">,</span>
    <span class="s2">&quot;tconv_nac&quot;</span><span class="p">:</span> <span class="n">tconv_nac</span><span class="p">,</span>
    <span class="s2">&quot;transformer&quot;</span><span class="p">:</span> <span class="n">transformer</span><span class="p">,</span>
    <span class="s2">&quot;transformer_tower&quot;</span><span class="p">:</span> <span class="n">transformer_tower</span><span class="p">,</span>
    <span class="s2">&quot;unet_conv&quot;</span><span class="p">:</span> <span class="n">unet_conv</span><span class="p">,</span>
    <span class="s2">&quot;unet_concat&quot;</span><span class="p">:</span> <span class="n">unet_concat</span><span class="p">,</span>
    <span class="s2">&quot;upper_tri&quot;</span><span class="p">:</span> <span class="n">upper_tri</span><span class="p">,</span>
    <span class="s2">&quot;wheeze_excite&quot;</span><span class="p">:</span> <span class="n">wheeze_excite</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">keras_func</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Conv1D&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span><span class="p">,</span>
    <span class="s2">&quot;Cropping1D&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Cropping1D</span><span class="p">,</span>
    <span class="s2">&quot;Cropping2D&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Cropping2D</span><span class="p">,</span>
    <span class="s2">&quot;Dense&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">,</span>
    <span class="s2">&quot;Flatten&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, David Kelly.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>